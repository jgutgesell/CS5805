[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html",
    "href": "posts/Probability Theory and Random Variables/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html",
    "href": "posts/Linear and Nonlinear Regression/index.html",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Unlike clustering which is an unsupervised technique, meaning that there are not labels or classes on the data, classification is the process of assigning data points to one of the predetermined classes given.\n\nOne clustering algorithm is Decision Trees, which learn decision rules from data features to build a tree that can be followed to determine the chosen class for a new data point.\n\n\nset.seed(222)\nindex &lt;- sample(2, nrow(iris), replace = TRUE, prob = c(0.7, 0.3))\ntrain &lt;- iris[index==1,]\ntest &lt;- iris[index==2,]\n\n\nlibrary(tree)\n\nirisTree &lt;- tree(Species ~ ., data = train)\nplot(irisTree, lwd = 2)\ntext(irisTree, cex = 0.7)\n\n\n\n\n\npredict_frame_iris &lt;- data.frame(test, prediction = predict(irisTree, test, type = \"class\"))\n\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\n(train_tab = table(predict_frame_iris$Species, predict_frame_iris$prediction))\n\n            \n             setosa versicolor virginica\n  setosa         18          0         0\n  versicolor      0         15         0\n  virginica       0          3        13\n\ntrain_con_mat = confusionMatrix(train_tab, positive = \"setosa\")\ntrain_con_mat\n\nConfusion Matrix and Statistics\n\n            \n             setosa versicolor virginica\n  setosa         18          0         0\n  versicolor      0         15         0\n  virginica       0          3        13\n\nOverall Statistics\n                                          \n               Accuracy : 0.9388          \n                 95% CI : (0.8313, 0.9872)\n    No Information Rate : 0.3673          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.9081          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            0.8333           1.0000\nSpecificity                 1.0000            1.0000           0.9167\nPos Pred Value              1.0000            1.0000           0.8125\nNeg Pred Value              1.0000            0.9118           1.0000\nPrevalence                  0.3673            0.3673           0.2653\nDetection Rate              0.3673            0.3061           0.2653\nDetection Prevalence        0.3673            0.3061           0.3265\nBalanced Accuracy           1.0000            0.9167           0.9583\n\n\nRandom Forest\n\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nrandomforest &lt;- randomForest(Species~., data=train, proximity=TRUE)\n\np2 &lt;- predict(randomforest, test)\nconfusionMatrix(p2, test$Species)\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         18          0         0\n  versicolor      0         15         3\n  virginica       0          0        13\n\nOverall Statistics\n                                          \n               Accuracy : 0.9388          \n                 95% CI : (0.8313, 0.9872)\n    No Information Rate : 0.3673          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.9081          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            1.0000           0.8125\nSpecificity                 1.0000            0.9118           1.0000\nPos Pred Value              1.0000            0.8333           1.0000\nNeg Pred Value              1.0000            1.0000           0.9167\nPrevalence                  0.3673            0.3061           0.3265\nDetection Rate              0.3673            0.3061           0.2653\nDetection Prevalence        0.3673            0.3673           0.2653\nBalanced Accuracy           1.0000            0.9559           0.9062\n\n\n\nplot(randomforest)"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/Anomaly and Outlier Detection/index.html",
    "href": "posts/Anomaly and Outlier Detection/index.html",
    "title": "Anomaly and Outlier Detection",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is an unsupervised learning method that divides data points into a number of groups based on similar properties. There are multiple different clustering methods vary in the distance calculations to determine the clusters. Some of these algorithms are:\n\nK-Means\nAffinity Propagation\nMean-Shift\nDBSCAN\nGaussian Mixtures\nSpectral Clustering\n\nWe will focus on DBSCAN - Density-Based Spacial Clustering of Applications With Noise.\nOne problem with some of the other clustering algorithms is that they only work well when the clusters are separate and compact. Noise and outliers can cause problems with other clustering methods. In real life datasets clusters could be arbitrary shapes and have noise and as we can see below DBSCAN is sometimes needed to be used instead of other clustering models such as K-Means.\n\nClusters are inherently high-density areas of space that are surrounded by lower-density space. When humans look at images we can identify the clusters easily, but some of these models go based on pure distance instead of looking at density and this is where DBSCAN can be especially useful in cases with more complex shapes and noise.\nDBSCAN Algorithm\n\nDefine Parameters\n\n\nEpsilon (eps) is the maximum distance between two points in order for them to be considered neighbors. If epsilon is too small, many points will be labeled as outliers, and if epsilon is too large we will not have definition in our clusters. We can use the k-distance graph to find our epsilon value.\nMinimum Points (MinPts) is the minimum number of neighbors within the epsilon radius. The minimum value of MinPts is 3, but as the dataset gets larger our MinPts paramter should also get larger.\n\n\nFind all neighbor points within eps and identify core points - the points that have more than MinPts within its eps radius.\nFor each core point if it has not been assigned a cluster, create a new cluster with that point.\nFind all density-connected points to the core point and assign to the same cluster. Two points are density-connected if there is a core point that contains both points within its eps radius.\nPoints that do not belong to a cluster are considered noise.\n\nExample with Code\n\nlibrary(factoextra)\n\nLoading required package: ggplot2\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\n# set random seed to make reproducable results \nset.seed(123456789)\n\n# extract x and y coordinates\nmultishapes &lt;- multishapes[, 1:2]\nplot(multishapes)\n\n\n\n\nIn this plot we can see that there are 5 different clusters: 2 ovals at the top of the image, 2 lines at the bottom left, and one dense cluster at the bottom right.\nLet’s see how K-Means would cluster this dataset:\n\nkm_res &lt;- kmeans(multishapes, 5, nstart = 25)\nplot(multishapes, col=km_res$cluster+1, main=\"K-means\")\n\n\n\n\nWe can see that this may not be the best possible clusters. We would expect to see the circles be one cluster each not split into to 3 different clusters across the 2 circles. K-Means did not separate the 2 lines into different clusters either.\nNow let’s see if DBSCAN does a better job:\nFirst we must decide our value of eps using the k-distance plot. We set k=5 since we are going to use minPts=5. The value of eps we want to use is where the elbow of the graph is.\n\nlibrary(dbscan)\n\n\nAttaching package: 'dbscan'\n\n\nThe following object is masked from 'package:stats':\n\n    as.dendrogram\n\nkNNdistplot(multishapes, k=5)\n\n\n\n\nWe see that our elbow is below 0.2, so we will use eps=0.15 for our inital DBSCAN algorithm.\n\ndbscan_res &lt;- dbscan(multishapes, eps = 0.15, minPts = 5)\nplot(multishapes, col=dbscan_res$cluster+1, main=\"DBSCAN\")\n\n\n\n\nNow the clusters look much more like how we would have expected. We also see the noisy points in black are not part of any cluster.\nParameter Effects\nLet’s see first hand how changing our 2 parameters effect the clusters we find.\n\nSmaller eps\n\n\ndbscan_res &lt;- dbscan(multishapes, eps = 0.05, minPts = 5)\nplot(multishapes, col=dbscan_res$cluster+1, main=\"DBSCAN\")\n\n\n\n\nWe see way too many of our points are outliers.\n\nLarger eps\n\n\ndbscan_res &lt;- dbscan(multishapes, eps = 0.45, minPts = 5)\nplot(multishapes, col=dbscan_res$cluster+1, main=\"DBSCAN\")\n\n\n\n\nWith the larger eps value we now only have 2 clusters, meaning we lose a lot of the details of our points.\n\nLarger minPts\n\n\ndbscan_res &lt;- dbscan(multishapes, eps = 0.15, minPts = 7)\nplot(multishapes, col=dbscan_res$cluster+1, main=\"DBSCAN\")\n\n\n\n\nNow we have a situation where there are now too many clusters and it is more complicated than is necessary."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS5805",
    "section": "",
    "text": "Classification\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nJulia Gutgesell\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2023\n\n\nJulia Gutgesell\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nJulia Gutgesell\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly and Outlier Detection\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nJulia Gutgesell\n\n\n\n\n\n\n  \n\n\n\n\nProbability Theory and Random Variables\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nJulia Gutgesell\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  }
]