[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is my machine learning blog for CS5805 at Virginia Tech. I am a senior at Virginia Tech finishing my Bachelor’s in Computational Modeling and Data Analytics while starting my Master’s in Computer Science with a concentration in Data Analytics & Artificial Intelligence."
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is an unsupervised learning method that divides data points into a number of groups based on similar properties. There are multiple different clustering methods vary in the distance calculations to determine the clusters. Some of these algorithms are:\n\nK-Means\nAffinity Propagation\nMean-Shift\nDBSCAN\nGaussian Mixtures\nSpectral Clustering\n\nWe will focus on DBSCAN - Density-Based Spacial Clustering of Applications With Noise.\nOne problem with some of the other clustering algorithms is that they only work well when the clusters are separate and compact. Noise and outliers can cause problems with other clustering methods. In real life datasets clusters could be arbitrary shapes and have noise and as we can see below DBSCAN is sometimes needed to be used instead of other clustering models such as K-Means.\n\nClusters are inherently high-density areas of space that are surrounded by lower-density space. When humans look at images we can identify the clusters easily, but some of these models go based on pure distance instead of looking at density and this is where DBSCAN can be especially useful in cases with more complex shapes and noise.\nDBSCAN Algorithm\n\nDefine Parameters\n\n\nEpsilon (eps) is the maximum distance between two points in order for them to be considered neighbors. If epsilon is too small, many points will be labeled as outliers, and if epsilon is too large we will not have definition in our clusters. We can use the k-distance graph to find our epsilon value.\nMinimum Points (MinPts) is the minimum number of neighbors within the epsilon radius. The minimum value of MinPts is 3, but as the dataset gets larger our MinPts paramter should also get larger.\n\n\nFind all neighbor points within eps and identify core points - the points that have more than MinPts within its eps radius.\nFor each core point if it has not been assigned a cluster, create a new cluster with that point.\nFind all density-connected points to the core point and assign to the same cluster. Two points are density-connected if there is a core point that contains both points within its eps radius.\nPoints that do not belong to a cluster are considered noise.\n\nExample with Code\n\nlibrary(factoextra)\n\n# set random seed to make reproducable results \nset.seed(123456789)\n\n# extract x and y coordinates\nmultishapes &lt;- multishapes[, 1:2]\nplot(multishapes)\n\n\n\n\nIn this plot we can see that there are 5 different clusters: 2 ovals at the top of the image, 2 lines at the bottom left, and one dense cluster at the bottom right.\nLet’s see how K-Means would cluster this dataset:\n\nkm_res &lt;- kmeans(multishapes, 5, nstart = 25)\nplot(multishapes, col=km_res$cluster+1, main=\"K-means\")\n\n\n\n\nWe can see that this may not be the best possible clusters. We would expect to see the circles be one cluster each not split into to 3 different clusters across the 2 circles. K-Means did not separate the 2 lines into different clusters either.\nNow let’s see if DBSCAN does a better job:\nFirst we must decide our value of eps using the k-distance plot. We set k=5 since we are going to use minPts=5. The value of eps we want to use is where the elbow of the graph is.\n\nlibrary(dbscan)\nkNNdistplot(multishapes, k=5)\n\n\n\n\nWe see that our elbow is below 0.2, so we will use eps=0.15 for our inital DBSCAN algorithm.\n\ndbscan_res &lt;- dbscan(multishapes, eps = 0.15, minPts = 5)\nplot(multishapes, col=dbscan_res$cluster+1, main=\"DBSCAN\")\n\n\n\n\nNow the clusters look much more like how we would have expected. We also see the noisy points in black are not part of any cluster.\nParameter Effects\nLet’s see first hand how changing our 2 parameters effect the clusters we find.\n\nSmaller eps\n\n\ndbscan_res &lt;- dbscan(multishapes, eps = 0.05, minPts = 5)\nplot(multishapes, col=dbscan_res$cluster+1, main=\"DBSCAN\")\n\n\n\n\nWe see way too many of our points are outliers.\n\nLarger eps\n\n\ndbscan_res &lt;- dbscan(multishapes, eps = 0.45, minPts = 5)\nplot(multishapes, col=dbscan_res$cluster+1, main=\"DBSCAN\")\n\n\n\n\nWith the larger eps value we now only have 2 clusters, meaning we lose a lot of the details of our points.\n\nLarger minPts\n\n\ndbscan_res &lt;- dbscan(multishapes, eps = 0.15, minPts = 7)\nplot(multishapes, col=dbscan_res$cluster+1, main=\"DBSCAN\")\n\n\n\n\nNow we have a situation where there are now too many clusters and it is more complicated than is necessary."
  },
  {
    "objectID": "posts/Anomaly and Outlier Detection/index.html",
    "href": "posts/Anomaly and Outlier Detection/index.html",
    "title": "Anomaly and Outlier Detection",
    "section": "",
    "text": "Anomaly and Outlier Detection are important steps of machine learning because without proper detection, we may be creating models that do not correctly model the population.\nAnomalies are data points which do not follow the expected pattern/distribution that the rest of the data follows. Anomalies are important because they can translate into relevant information in many cases. For example, if we are looking at all credit card transactions, fraudulent transactions may be caught with anomaly detection since they could be unusual amounts or going to unusual businesses. Through anomaly detection we may be able to find new features to add to the model or a new model.\nOutliers are observations that have a rare chance of occurring within the dataset because they are far from the other points. Outliers can occur because of human or experimental error, but sometimes we do not know the reason for the outlier. Outlier detection is important because outliers can skew our model and affect the accuracy for future predictions.\n\nlibrary(dbscan)\nlibrary(factoextra)\nset.seed(123456789)\nmultishapes &lt;- multishapes[, 1:2]\nplot(multishapes)\n\n\n\n\nIt is hard for us to know which of these points are going to be outliers, and it is not a good idea to remove outliers for no reason. A model that is robust to outliers is DBSCAN since it only includes points that have a minimum number of points, at least 3, in a radius surrounding the point.\n\ndbscan_res &lt;- dbscan(multishapes, eps = 0.15, minPts = 5)\nplot(multishapes, col=dbscan_res$cluster+1, main=\"DBSCAN\")\n\n\n\n\nAll the points in black are considered outliers. We can see that some of the points we may have initially thought were outliers are, but there are some points that could have been hard to indentify just using the human eye. This way we don’t delete our outliers from our dataset, but we do not allow the outliers to lower the accuracy of our model."
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Unlike clustering which is an unsupervised technique, meaning that there are not labels or classes on the data, classification is the process of assigning data points to one of the predetermined classes given.\n\nOne type of clustering algorithm is Decision Trees, which learn decision rules from data features to build a tree that can be followed to determine the chosen class for a new data point. It uses if-then rules to determine what action to take at a specific node/situation.\n Iris Example\n\nset.seed(222)\nindex &lt;- sample(2, nrow(iris), replace = TRUE, prob = c(0.7, 0.3))\ntrain &lt;- iris[index==1,]\ntest &lt;- iris[index==2,]\n\n\nlibrary(tree)\n\nirisTree &lt;- tree(Species ~ ., data = train)\nplot(irisTree, lwd = 2)\ntext(irisTree, cex = 0.7)\n\n\n\n\n\npredict_frame_iris &lt;- data.frame(test, prediction = predict(irisTree, test, type = \"class\"))\n\nlibrary(caret)\n(train_tab = table(predict_frame_iris$Species, predict_frame_iris$prediction))\n\n            \n             setosa versicolor virginica\n  setosa         18          0         0\n  versicolor      0         15         0\n  virginica       0          3        13\n\ntrain_con_mat = confusionMatrix(train_tab, positive = \"setosa\")\ntrain_con_mat\n\nConfusion Matrix and Statistics\n\n            \n             setosa versicolor virginica\n  setosa         18          0         0\n  versicolor      0         15         0\n  virginica       0          3        13\n\nOverall Statistics\n                                          \n               Accuracy : 0.9388          \n                 95% CI : (0.8313, 0.9872)\n    No Information Rate : 0.3673          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.9081          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            0.8333           1.0000\nSpecificity                 1.0000            1.0000           0.9167\nPos Pred Value              1.0000            1.0000           0.8125\nNeg Pred Value              1.0000            0.9118           1.0000\nPrevalence                  0.3673            0.3673           0.2653\nDetection Rate              0.3673            0.3061           0.2653\nDetection Prevalence        0.3673            0.3061           0.3265\nBalanced Accuracy           1.0000            0.9167           0.9583\n\n\nRandom Forest\nDecision Trees tend to overfit the data which can lead to a worse out of sample performance. To avoid this problem we can use a specific type of tree algorithm a Random Forest. Random Forest, like the name suggests, use many samples of the data and form a decision tree on each sample making a “forest” of many trees which are averaged to form the final model. Because of the many samples and averaging, the model is less prone to overfitting.\n\nlibrary(randomForest)\n\nrandomforest &lt;- randomForest(Species~., data=train, proximity=TRUE)\n\np2 &lt;- predict(randomforest, test)\nconfusionMatrix(p2, test$Species)\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         18          0         0\n  versicolor      0         15         3\n  virginica       0          0        13\n\nOverall Statistics\n                                          \n               Accuracy : 0.9388          \n                 95% CI : (0.8313, 0.9872)\n    No Information Rate : 0.3673          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.9081          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            1.0000           0.8125\nSpecificity                 1.0000            0.9118           1.0000\nPos Pred Value              1.0000            0.8333           1.0000\nNeg Pred Value              1.0000            1.0000           0.9167\nPrevalence                  0.3673            0.3061           0.3265\nDetection Rate              0.3673            0.3061           0.2653\nDetection Prevalence        0.3673            0.3673           0.2653\nBalanced Accuracy           1.0000            0.9559           0.9062\n\n\n\nplot(randomforest)"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html",
    "href": "posts/Linear and Nonlinear Regression/index.html",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "Linear Regression\nLinear regression is the process of creating a line that best fits the data that can be used to predict the value of the response variable for new data.\nIf we want to fit a linear model to our data it is important that we make sure that there is a linear relationship to model.\n\nlibrary(ggplot2)\nlibrary(MASS)\nggplot(cats, aes(x=Hwt, y=Bwt)) + geom_point() + theme_bw() +\n  labs(title = \"Body Weight Versus Heart Weight in Cats\")\n\n\n\n\nIn this example of body weight and heart weights of cats, we can use a scatterplot to visualize the data first to see that we have a linear relationship that can be modeled. When we have more than just 2 variables that we are considering to use in our model we can use a scatterplot matrix to visualize all the scatterplots at once.\nAnother tool to determine the linear association between two variables is correlation. The closer that the correlation coefficient r is to -1 or 1, the stronger the linear relationship. Something to keep in mind is that correlation is not a complete indicator of a relationship between the variables. If there is a polynomial relationship the correlation will be low, as well as outliers can make it seem like there is a linear relationship when there is not.\nRegression Equations\nIf we see that there is a linear relationship that we can model, the next step is to estimate that line, called the least-squares or fitted regression line.\nThe true mean response variable \\(Y\\) conditioned on \\(X\\) can be written:\n\\(\\mu_{Y|X} = \\beta_0 + \\beta_1 X\\)\nThe statistical linear model, which looks at individual response variables \\(Y_i\\) can be written:\n\\(Y_i = \\beta_0 + \\beta_1 X + \\varepsilon_i\\)\nWhere \\(\\beta_i\\) is the true regression coefficients that are unknown and \\(\\varepsilon_i \\sim N(0,\\sigma^2)\\) is the random error or residuals.\nFinding Regression Line in R\n\nmodel &lt;- lm(Bwt ~ Hwt, data=cats)\nmodel$coefficients\n\n(Intercept)         Hwt \n  1.0196367   0.1602902 \n\n\nThe line that our linear model found is Body Weight = 1.02 + 0.16 Heart Weight.\nWe can plot our line over our data to see how well it fits.\n\nggplot(cats, aes(x=Hwt, y=Bwt)) + geom_point() + theme_bw() +\n  geom_smooth(method = \"lm\", se = F) + \n  labs(title = \"Body Weight Versus Heart Weight in Cats\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nIt looks like a decent fit, but we can also look at the full estimate table of our model to see our \\(R^2\\) and the significance of our predictors.\n\nsummary(model)\n\n\nCall:\nlm(formula = Bwt ~ Hwt, data = cats)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.58283 -0.22140 -0.00879  0.20825  0.91717 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.019637   0.108428   9.404   &lt;2e-16 ***\nHwt         0.160290   0.009944  16.119   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2895 on 142 degrees of freedom\nMultiple R-squared:  0.6466,    Adjusted R-squared:  0.6441 \nF-statistic: 259.8 on 1 and 142 DF,  p-value: &lt; 2.2e-16\n\n\nWe see that heart weight is significant and we should use it in our model, later we will look at what to do if we find that some of our variables are not significant. Our \\(R^2\\) means that 65% of the variance of body weight can be explained by heart weight. That means there are other variables that we don’t have that also explain the body weight.\nChecking Assumptions\nThere are a few key assumptions in order to use a linear model\n\nThe mean of \\(Y\\) is linear in \\(X\\)\nThe errors \\(\\varepsilon_i\\) are independent and identically distributed\n\nTo check our linearity assumption we can look at the original plot to make sure it looks linear as well as look at a plot of the residuals versus the predicted values from our model.\n\nplot(rstudent(model) ~ fitted(model))\n\n\n\n\nSince we don’t see any big curvature, we can say that this model meets the linearity assumption.\nTo check for normality we plot the normal probability plot for the studentized residuals.\n\nqqnorm(rstudent(model))\nqqline(rstudent(model))\n\n\n\n\nTo check for constant variance across the observations we plot the residuals versus the fitted values. If we see any fan shape or pattern then we have violated the assumption.\n\nplot(rstudent(model) ~ fitted(model))\nabline(h=0)\n\n\n\n\nWhen errors are normally distributed, the least-squares method we showed is the best method. When errors follow another distribution we should use different models.\nRobust Regression\nIf our data has outliers, we can use robust regression to protect their influence on estimated parameters such as our regression coefficients. The main idea is we use a weighted least-squares where we down weight the outliers to minimize their effect on the coefficients.\nWhat makes robust regression different is the M-estimator that is used. M-estimators are a generalization of least-squares where our \\(\\beta\\) is chosen to minimize a loss function. We use a function of the residuals \\(\\rho\\) which has multiple options. We will look at 2 different \\(\\rho\\) functions: Huber and Bi-weight.\nWe are going to add outliers to the cats dataset we used for simple linear regression above to show how we would use robust regression to minimize their effect.\n\nnewobs &lt;- data.frame( \"Sex\" = factor(c(\"F\",\"F\", \"M\", \"F\", \"M\")),\n\"Bwt\" = c(3.5, 3.2, 3.1, 3.4, 2.9),\n\"Hwt\" = c(30, 27, 22, 25, 18.5))\n\nmycats &lt;- rbind(cats, newobs)\n\nggplot(mycats, aes(x = Bwt, y = Hwt)) + geom_point() + theme_bw() + geom_point(data = newobs, aes(x = Bwt, y = Hwt), color = \"red\", size = 2)\n\n\n\n\nLet’s compare how ordinary least-squares, robust regression using Huber method, and robust regression using Bi-square method.\n\nmodel_OLS &lt;- lm(Hwt ~ Bwt, data = mycats)\nmodel_huber &lt;- rlm(Hwt ~ Bwt, data = mycats, psi = psi.huber)\nmodel_bisquare &lt;- rlm(Hwt ~ Bwt, data = mycats, psi = psi.bisquare)\n\n\nggplot(mycats, aes(x = Bwt, y = Hwt)) + geom_point() + theme_bw() + geom_point(data = newobs, aes(x = Bwt, y = Hwt), color = \"red\", size = 2) + geom_smooth(method = \"lm\", se = F, color = \"red\") +\ngeom_smooth(method = \"rlm\", se = F, color = \"magenta\") +\ngeom_smooth(data = cats, aes(x = Bwt, y = Hwt), method = \"lm\", se=F, color = \"blue\") +\ngeom_line(data = data.frame(\"Bwt\" = mycats$Bwt, \"Hwt\" = model_bisquare$fitted.values),\ncolor = \"green\", size = 1.05)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nblue line: ordinary least-squares on original data\nred line: ordinary least-squares with outliers\nmagenta line: robust linear model with Huber method\ngreen lineL: robust linear model with Bi-square method"
  },
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html",
    "href": "posts/Probability Theory and Random Variables/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "Probability theory is an important aspect of many machine learning algorithms since in reality there are very few things that we know with complete certainty.\nTwo important properties of probability are that probabilities are between 0 and 1, with 0 being impossible and 1 being certain, and that the some of all probabilities of an event have to equal 1.\n\nTo describe the probabilities that random variable \\(x\\) can take on each specific outcome of the event we can use a probability distribution. There are two different types of probability distributions: discrete and continuous.\nDiscrete Distributions\nDiscrete distributions are when the outcomes are finite like heads or tails of a coin. A Bernoulli distribution is an example of a discrete distribution where there are only 2 potential outcomes. The probability mass function of a Bernoulli distribution of a fair coin flip is P(x) = {0.5 if x = heads, 0.5 if x = tails}.\nContinuous Distributions\nContinuous distributions have infinitely many outcomes, for example heights. One well known continuous distribution is the normal distribution which can be visualized with the bell curve.\n\n\n\n\n\nUnlike discrete distributions where the value of the probability mass function at \\(x\\) is the probability of of \\(x\\), in the continuous distribution since there are infinitely many outcomes the probability of X = \\(x\\) is actually 0. All of our probabilities must sum to 1, when looking at the normal distribution curve that means that the area under the curve must also be 1. So instead of using a summation, we look at intervals in the continuous case.\n\nSo instead of looking at the probability of an event at a specific value which we now know is 0, we can use ranges of X and calculate the area under the curve for those ranges to find meaningful probabilities in the continuous case. We can use something called the cumulative distribution function which takes the probability density function we saw above and calculates the area under the curve to the left of \\(x\\). This is what the CDF of the normal distribution looks like:\n\n\n\n\n\nNow we can see probabilities much easier such as \\(P(x &lt; -3)\\) is nearly 0 and \\(P(x &lt; 3)\\) is nearly 1.\nConditional Probability Distributions\nIn order to understand Bayes’ Rule we first must understand conditional probabilities. Oftentimes, the probabilities we are interested in are dependent on another event. The conditional probability of x given y is:\n\nBy multiplying by \\(P(y)\\) we get the chain rule of probability: \\(P(x,y) = P(x|y) P(y)\\)\nBayesian Probability and Bayes’ Rule\nA well known probability philosophy is Bayesian Probability. Unlike an objective perspective where known beliefs are used, a subjective perspective where the observer’s own learnings and experience are used, Bayesian probability combines prior beliefs with observations. We can use Bayes’ Rule to calculate our probabilities.\nBack to our chain rule, we can write equivalent statments\n\n\\(P(x,y) = P(x|y) P(y)\\)\n\\(P(x,y) = P(y|x) P(x)\\)\n\nIf we set these to be equal and divide by \\(P(y)\\) we get Bayes’ Rule:\n\nThis rule allows us update our beliefs as we gain more observations. The prior probability, which is what initial probability of an event, and the posterior probability is the probability after adding in our new observation.\nNaive Bayes\nNaive Bayes use Bayes’ Rule but with a couple key assumptions:\n\nPredictors are conditionally independent\nAll features contribute equally to the outcome\n\nAlthough this may not lead to the most complete and accurate model in real life, it allows us to simplify the problem.\n\nlibrary(caret)\nlibrary(e1071)\nlibrary(caTools)\n\n\n# split data into train and test splits\nset.seed(222)\nindex &lt;- sample(2, nrow(iris), replace = TRUE, prob = c(0.7, 0.3))\ntrain &lt;- iris[index==1,]\ntest &lt;- iris[index==2,]\n\n\nbayes &lt;- naiveBayes(Species ~ ., data = train)\nbayes\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n    setosa versicolor  virginica \n 0.3168317  0.3465347  0.3366337 \n\nConditional probabilities:\n            Sepal.Length\nY                [,1]      [,2]\n  setosa     5.021875 0.3526255\n  versicolor 5.874286 0.5505841\n  virginica  6.558824 0.6339475\n\n            Sepal.Width\nY                [,1]      [,2]\n  setosa     3.475000 0.3698300\n  versicolor 2.768571 0.2997758\n  virginica  2.941176 0.3026325\n\n            Petal.Length\nY                [,1]      [,2]\n  setosa     1.475000 0.1703886\n  versicolor 4.211429 0.5166472\n  virginica  5.538235 0.5570565\n\n            Petal.Width\nY                [,1]      [,2]\n  setosa     0.240625 0.1011526\n  versicolor 1.320000 0.2111593\n  virginica  2.029412 0.2480584\n\n\nWe see that our Naive Bayes function calculates our prior probabilities and the conditional probabilities for our predictors.\n\nprediction &lt;- predict(bayes, newdata = test)\n\nconfusion &lt;- table(test$Species, prediction)\nconfusionMatrix(confusion)\n\nConfusion Matrix and Statistics\n\n            prediction\n             setosa versicolor virginica\n  setosa         18          0         0\n  versicolor      0         15         0\n  virginica       0          2        14\n\nOverall Statistics\n                                         \n               Accuracy : 0.9592         \n                 95% CI : (0.8602, 0.995)\n    No Information Rate : 0.3673         \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      \n                                         \n                  Kappa : 0.9387         \n                                         \n Mcnemar's Test P-Value : NA             \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            0.8824           1.0000\nSpecificity                 1.0000            1.0000           0.9429\nPos Pred Value              1.0000            1.0000           0.8750\nNeg Pred Value              1.0000            0.9412           1.0000\nPrevalence                  0.3673            0.3469           0.2857\nDetection Rate              0.3673            0.3061           0.2857\nDetection Prevalence        0.3673            0.3061           0.3265\nBalanced Accuracy           1.0000            0.9412           0.9714\n\n\nWe see that we get a model accuracy of 95% which is very good.\nImages used can be found here"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS5805",
    "section": "",
    "text": "Clustering\n\n\n\n\n\n\nJulia Gutgesell\n\n\nNov 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nClassification\n\n\n\n\n\n\nJulia Gutgesell\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nProbability Theory and Random Variables\n\n\n\n\n\n\nJulia Gutgesell\n\n\nNov 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\n\nJulia Gutgesell\n\n\nNov 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nAnomaly and Outlier Detection\n\n\n\n\n\n\nJulia Gutgesell\n\n\nNov 24, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  }
]