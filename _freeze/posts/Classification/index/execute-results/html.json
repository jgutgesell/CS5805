{
  "hash": "e7245802cfd51cbadde6b4e5ed6a534e",
  "result": {
    "markdown": "---\ntitle: \"Classification\"\nauthor: \"Julia Gutgesell\"\ndate: \"2023-11-19\"\n---\n\n\nUnlike clustering which is an unsupervised technique, meaning that there are not labels or classes on the data, classification is the process of assigning data points to one of the predetermined classes given.\n\n![](classification.png)\n\nOne type of clustering algorithm is **Decision Trees**, which learn decision rules from data features to build a tree that can be followed to determine the chosen class for a new data point. It uses if-then rules to determine what action to take at a specific node/situation.\n\n![](tree.png) **Iris Example**\n\nFirst we will split out dataset into the training and testing datasets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(222)\nindex <- sample(2, nrow(iris), replace = TRUE, prob = c(0.7, 0.3))\ntrain <- iris[index==1,]\ntest <- iris[index==2,]\n```\n:::\n\n\nUsing the tree library we can train our decision tree with our training dataset and look at our tree. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tree)\n\nirisTree <- tree(Species ~ ., data = train)\nplot(irisTree, lwd = 2)\ntext(irisTree, cex = 0.7)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\nIf we wanted to classify new data points by hand we could follow the tree down based on its values. For small trees like this one it is possible, but for larger datasets with many more variables it will become very complicated. \n\nIf we look closely at our tree we will notice that our tree only uses the Petal length and width to classify the points, since the Sepal width leads to Versicolor both times. We can visualize our tree with the test points to see how they would be classified.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict_frame_iris <- data.frame(test, prediction =\n                                   predict(irisTree, test, type = \"class\"))\n\nlibrary(caret)\ntrain_tab = table(predict_frame_iris$Species, predict_frame_iris$prediction)\ntrain_con_mat = confusionMatrix(train_tab, positive = \"setosa\")\ntrain_con_mat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n            \n             setosa versicolor virginica\n  setosa         18          0         0\n  versicolor      0         15         0\n  virginica       0          3        13\n\nOverall Statistics\n                                          \n               Accuracy : 0.9388          \n                 95% CI : (0.8313, 0.9872)\n    No Information Rate : 0.3673          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.9081          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            0.8333           1.0000\nSpecificity                 1.0000            1.0000           0.9167\nPos Pred Value              1.0000            1.0000           0.8125\nNeg Pred Value              1.0000            0.9118           1.0000\nPrevalence                  0.3673            0.3673           0.2653\nDetection Rate              0.3673            0.3061           0.2653\nDetection Prevalence        0.3673            0.3061           0.3265\nBalanced Accuracy           1.0000            0.9167           0.9583\n```\n:::\n:::\n\n\nWe can see that there are 3 points in our test set that were labeled versicolor when they are actually virginicas. We can transform our tree into 2-dimensional space to see our misclassified points.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nggplot(test, aes(x = Petal.Length, y = Petal.Width, color = Species)) +\ngeom_point() + geom_rect(aes(xmin = 0, xmax = 2.45, ymin = 0, ymax = 2.6 ), \n    fill = NA, color = \"red\", alpha = 0.25) +\ngeom_rect(aes(xmin = 2.45, xmax = 7, ymin = 0, ymax = 1.65), \n    fill = NA, color = \"green\", alpha = 0.25) +\ngeom_rect(aes(xmin = 2.45, xmax = 7, ymin = 1.65, ymax = 2.6), \n    fill = NA, color = \"blue\", alpha = 0.25) +\nannotate(geom = \"text\", x = 1.5, y = 0.8, label = \"setosa\", color = \"red\") +\nannotate(geom = \"text\", x = 4.5, y = 0.8, label = \"versicolor\", color = \"green\") +\nannotate(geom = \"text\", x = 3.5, y = 2, label = \"virginica\", color = \"blue\") +\nscale_x_continuous(breaks = c(0, 2.45, 4.95, 6.9)) + \nscale_y_continuous(breaks = c(0, 1.75, 2.5))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n**Random Forest**\n\nDecision Trees tend to overfit the data which can lead to a worse out of sample performance. To avoid this problem we can use a specific type of tree algorithm a **Random Forest**. Random Forest, like the name suggests, use many samples of the data and form a decision tree on each sample making a \"forest\" of many trees which are averaged to form the final model. Because of the many samples and averaging, the model is less prone to overfitting.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(randomForest)\n\nrandomforest <- randomForest(Species~., data=train, proximity=TRUE)\n\np2 <- predict(randomforest, test)\nconfusionMatrix(p2, test$Species)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         18          0         0\n  versicolor      0         15         3\n  virginica       0          0        13\n\nOverall Statistics\n                                          \n               Accuracy : 0.9388          \n                 95% CI : (0.8313, 0.9872)\n    No Information Rate : 0.3673          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.9081          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            1.0000           0.8125\nSpecificity                 1.0000            0.9118           1.0000\nPos Pred Value              1.0000            0.8333           1.0000\nNeg Pred Value              1.0000            1.0000           0.9167\nPrevalence                  0.3673            0.3061           0.3265\nDetection Rate              0.3673            0.3061           0.2653\nDetection Prevalence        0.3673            0.3673           0.2653\nBalanced Accuracy           1.0000            0.9559           0.9062\n```\n:::\n:::\n\n\nWe see that in this situation we get the same results as our decision tree. This means that our iris example is not prone to the overfitting that can occur in decision trees. However, for other datasets the issue of overfitting may exist and that is when we should use random forest.\n\nWe can also plot our error rate of our random forest. Where the red line is for Setosa, blue is Versicolor, and green is Virginica. Since this is not a situation that random forest improves upon our accuracy we can see that the error is lowest between 0 and 100 trees. \n\n::: {.cell}\n\n```{.r .cell-code}\nplot(randomforest)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nWe have seen in this post how to classify a dataset using Decision Trees and Random Forest. We learned that a Decision Tree has combinations of decisions to classify our data and Random Forest is a combination of Decision Trees averaged to reduce overfitting. We can chose between the two depending on our needs. Decision Trees are faster but Random Forests tend to be more accurate. ",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}