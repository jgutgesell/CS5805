{
  "hash": "14c1f4404d96c242a7c8d0f6da2dbc4f",
  "result": {
    "markdown": "---\ntitle: \"Linear and Nonlinear Regression\"\nauthor: \"Julia Gutgesell\"\ndate: \"2023-11-22\"\n---\n\n\nLinear regression is the process of creating a line that best fits the data that can be used to predict the value of the response variable for new data.\n\nIf we want to fit a linear model to our data it is important that we make sure that there is a linear relationship to model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(MASS)\nggplot(cats, aes(x=Hwt, y=Bwt)) + geom_point() + theme_bw() +\n  labs(title = \"Body Weight Versus Heart Weight in Cats\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nIn this example of body weight and heart weights of cats, we can use a scatterplot to visualize the data first to see that we have a linear relationship that can be modeled. When we have more than just 2 variables that we are considering to use in our model we can use a scatterplot matrix to visualize all the scatterplots at once.\n\nAnother tool to determine the linear association between two variables is **correlation**. The closer that the correlation coefficient **r** is to -1 or 1, the stronger the linear relationship. Something to keep in mind is that correlation is not a complete indicator of a relationship between the variables. If there is a polynomial relationship the correlation will be low, as well as outliers can make it seem like there is a linear relationship when there is not.\n\n**Regression Equations**\n\nIf we see that there is a linear relationship that we can model, the next step is to estimate that line, called the least-squares or fitted regression line.\n\nThe true mean response variable $Y$ conditioned on $X$ can be written:\n\n$\\mu_{Y|X} = \\beta_0 + \\beta_1 X$\n\nThe statistical linear model, which looks at individual response variables $Y_i$ can be written:\n\n$Y_i = \\beta_0 + \\beta_1 X + \\varepsilon_i$\n\nWhere $\\beta_i$ is the true regression coefficients that are unknown and $\\varepsilon_i \\sim N(0,\\sigma^2)$ is the random error or residuals.\n\n**Finding Regression Line in R**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(Bwt ~ Hwt, data=cats)\nmodel$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)         Hwt \n  1.0196367   0.1602902 \n```\n:::\n:::\n\n\nThe line that our linear model found is **Body Weight = 1.02 + 0.16 Heart Weight**.\n\nWe can plot our line over our data to see how well it fits.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(cats, aes(x=Hwt, y=Bwt)) + geom_point() + theme_bw() +\n  geom_smooth(method = \"lm\", se = F) + \n  labs(title = \"Body Weight Versus Heart Weight in Cats\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nIt looks like a decent fit, but we can also look at the full estimate table of our model to see our $R^2$ and the significance of our predictors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Bwt ~ Hwt, data = cats)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.58283 -0.22140 -0.00879  0.20825  0.91717 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 1.019637   0.108428   9.404   <2e-16 ***\nHwt         0.160290   0.009944  16.119   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2895 on 142 degrees of freedom\nMultiple R-squared:  0.6466,\tAdjusted R-squared:  0.6441 \nF-statistic: 259.8 on 1 and 142 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nWe see that heart weight is significant and we should use it in our model, later we will look at what to do if we find that some of our variables are not significant. Our $R^2$ means that 65% of the variance of body weight can be explained by heart weight. That means there are other variables that we don't have that also explain the body weight.\n\n**Checking Assumptions**\n\nThere are a few key assumptions in order to use a linear model\n\n-   The mean of $Y$ is linear in $X$\n-   The errors $\\varepsilon_i$ are independent and identically distributed\n\nTo check our linearity assumption we can look at the original plot to make sure it looks linear as well as look at a plot of the residuals versus the predicted values from our model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(rstudent(model) ~ fitted(model))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nSince we don't see any big curvature, we can say that this model meets the linearity assumption.\n\nTo check for normality we plot the normal probability plot for the studentized residuals.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqqnorm(rstudent(model))\nqqline(rstudent(model))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nWe see some slight deviation from the line at the beginning and end, but it is not very much so we can assume normality is satisfied.\n\nTo check for constant variance across the observations we plot the residuals versus the fitted values. If we see any fan shape or pattern then we have violated the assumption.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(rstudent(model) ~ fitted(model))\nabline(h=0)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nWhen errors are normally distributed as we see above, the least-squares method we showed is the best method. When errors follow another distribution we should use different models.\n\n**Robust Regression**\n\nIf our data has outliers, we can use robust regression to protect their influence on estimated parameters such as our regression coefficients. The main idea is we use a weighted least-squares where we down weight the outliers to minimize their effect on the coefficients.\n\nWhat makes robust regression different is the **M-estimator** that is used. M-estimators are a generalization of least-squares where our $\\beta$ is chosen to minimize a loss function. We use a function of the residuals $\\rho$ which has multiple options. We will look at 2 different $\\rho$ functions: **Huber and Bi-weight**.\n\nWe are going to add outliers to the cats dataset we used for simple linear regression above to show how we would use robust regression to minimize their effect.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnewobs <- data.frame( \"Sex\" = factor(c(\"F\",\"F\", \"M\", \"F\", \"M\")),\n\"Bwt\" = c(3.5, 3.2, 3.1, 3.4, 2.9),\n\"Hwt\" = c(30, 27, 22, 25, 18.5))\n\nmycats <- rbind(cats, newobs)\n\nggplot(mycats, aes(x = Bwt, y = Hwt)) + geom_point() + theme_bw() + \n  geom_point(data = newobs, aes(x = Bwt, y = Hwt), color = \"red\", size = 2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nLet's compare how ordinary least-squares, robust regression using Huber method, and robust regression using Bi-square method.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_OLS <- lm(Hwt ~ Bwt, data = mycats)\nmodel_huber <- rlm(Hwt ~ Bwt, data = mycats, psi = psi.huber)\nmodel_bisquare <- rlm(Hwt ~ Bwt, data = mycats, psi = psi.bisquare)\n\n\nggplot(mycats, aes(x = Bwt, y = Hwt)) + geom_point() + theme_bw() + \n  geom_point(data = newobs, aes(x = Bwt, y = Hwt), color = \"red\", size = 2) +   \n  geom_smooth(method = \"lm\", se = F, color = \"red\") +\n  geom_smooth(method = \"rlm\", se = F, color = \"magenta\") +\n  geom_smooth(data = cats, aes(x = Bwt, y = Hwt), method = \"lm\", se=F, color = \"blue\") +\n  geom_line(data = data.frame(\"Bwt\" = mycats$Bwt, \"Hwt\" = model_bisquare$fitted.values),\n            color = \"green\", linewidth = 1.05)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n-   blue line: ordinary least-squares on original data\n-   red line: ordinary least-squares with outliers\n-   magenta line: robust linear model with Huber method\n-   green line: robust linear model with Bi-square method\n\nWe can see that the red line, which is the one if we did ordinary least-squares on our data with the addition of the outliers that it would not fit the data as well as the other lines. We do see that all the other 3 lines are very close together meaning both Huber method and Bi-square method do a good job at returning the line to where it would be without outliers. This is important since we do not want our outliers to worsen our models and their predictions since we want to predict normal data not outlier data.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}