{
  "hash": "e4a2d0599fe8edba7f90c690363a2bad",
  "result": {
    "markdown": "---\ntitle: \"Probability Theory and Random Variables\"\nauthor: \"Julia Gutgesell\"\ndate: \"2023-11-21\"\n---\n\n\nProbability theory is an important aspect of many machine learning algorithms since in reality there are very few things that we know with complete certainty.\n\nTwo important properties of probability are that probabilities are between 0 and 1, with 0 being impossible and 1 being certain, and that the some of all probabilities of an event have to equal 1.\n\n![](axioms.png)\n\nTo describe the probabilities that random variable $x$ can take on each specific outcome of the event we can use a **probability distribution**. There are two different types of probability distributions: discrete and continuous.\n\n**Discrete Distributions**\n\nDiscrete distributions are when the outcomes are finite like heads or tails of a coin. A Bernoulli distribution is an example of a discrete distribution where there are only 2 potential outcomes. The probability mass function of a Bernoulli distribution of a fair coin flip is P(x) = {0.5 if x = heads, 0.5 if x = tails}.\n\n**Continuous Distributions**\n\nContinuous distributions have infinitely many outcomes, for example heights. One well known continuous distribution is the normal distribution which can be visualized with the bell curve.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nUnlike discrete distributions where the value of the probability mass function at $x$ is the probability of of $x$, in the continuous distribution since there are infinitely many outcomes the probability of X = $x$ is actually 0. All of our probabilities must sum to 1, when looking at the normal distribution curve that means that the area under the curve must also be 1. So instead of using a summation, we look at intervals in the continuous case.\n\n![](integral.png)\n\nSo instead of looking at the probability of an event at a specific value which we now know is 0, we can use ranges of X and calculate the area under the curve for those ranges to find meaningful probabilities in the continuous case. We can use something called the cumulative distribution function which takes the probability density function we saw above and calculates the area under the curve to the left of $x$. This is what the CDF of the normal distribution looks like:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nNow we can see probabilities much easier such as $P(x < -3)$ is nearly 0 and $P(x < 3)$ is nearly 1.\n\n**Conditional Probability Distributions**\n\nIn order to understand Bayes' Rule we first must understand conditional probabilities. Oftentimes, the probabilities we are interested in are dependent on another event. The conditional probability of **x given y** is:\n\n![](conditional.png)\n\nBy multiplying by $P(y)$ we get the **chain rule** of probability: $P(x,y) = P(x|y) P(y)$\n\n**Bayesian Probability and Bayes' Rule**\n\nA well known probability philosophy is **Bayesian Probability**. Unlike an objective perspective where known beliefs are used, a subjective perspective where the observer's own learnings and experience are used, Bayesian probability combines prior beliefs with observations. We can use Bayes' Rule to calculate our probabilities.\n\nBack to our chain rule, we can write equivalent statments\n\n-   $P(x,y) = P(x|y) P(y)$\n-   $P(x,y) = P(y|x) P(x)$\n\nIf we set these to be equal and divide by $P(y)$ we get Bayes' Rule:\n\n![](Bayes.png)\n\nThis rule allows us update our beliefs as we gain more observations. The prior probability, which is what initial probability of an event, and the posterior probability is the probability after adding in our new observation.\n\n**Naive Bayes**\n\nNaive Bayes use Bayes' Rule but with a couple key assumptions:\n\n-   Predictors are conditionally independent\n-   All features contribute equally to the outcome\n\nAlthough this may not lead to the most complete and accurate model in real life, it allows us to simplify the problem.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\nlibrary(e1071)\nlibrary(caTools)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# split data into train and test splits\nset.seed(222)\nindex <- sample(2, nrow(iris), replace = TRUE, prob = c(0.7, 0.3))\ntrain <- iris[index==1,]\ntest <- iris[index==2,]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbayes <- naiveBayes(Species ~ ., data = train)\nbayes\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n    setosa versicolor  virginica \n 0.3168317  0.3465347  0.3366337 \n\nConditional probabilities:\n            Sepal.Length\nY                [,1]      [,2]\n  setosa     5.021875 0.3526255\n  versicolor 5.874286 0.5505841\n  virginica  6.558824 0.6339475\n\n            Sepal.Width\nY                [,1]      [,2]\n  setosa     3.475000 0.3698300\n  versicolor 2.768571 0.2997758\n  virginica  2.941176 0.3026325\n\n            Petal.Length\nY                [,1]      [,2]\n  setosa     1.475000 0.1703886\n  versicolor 4.211429 0.5166472\n  virginica  5.538235 0.5570565\n\n            Petal.Width\nY                [,1]      [,2]\n  setosa     0.240625 0.1011526\n  versicolor 1.320000 0.2111593\n  virginica  2.029412 0.2480584\n```\n:::\n:::\n\n\nWe see that our Naive Bayes function calculates our prior probabilities and the conditional probabilities for our predictors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction <- predict(bayes, newdata = test)\n\nconfusion <- table(test$Species, prediction)\nconfusionMatrix(confusion)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n            prediction\n             setosa versicolor virginica\n  setosa         18          0         0\n  versicolor      0         15         0\n  virginica       0          2        14\n\nOverall Statistics\n                                         \n               Accuracy : 0.9592         \n                 95% CI : (0.8602, 0.995)\n    No Information Rate : 0.3673         \n    P-Value [Acc > NIR] : < 2.2e-16      \n                                         \n                  Kappa : 0.9387         \n                                         \n Mcnemar's Test P-Value : NA             \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            0.8824           1.0000\nSpecificity                 1.0000            1.0000           0.9429\nPos Pred Value              1.0000            1.0000           0.8750\nNeg Pred Value              1.0000            0.9412           1.0000\nPrevalence                  0.3673            0.3469           0.2857\nDetection Rate              0.3673            0.3061           0.2857\nDetection Prevalence        0.3673            0.3061           0.3265\nBalanced Accuracy           1.0000            0.9412           0.9714\n```\n:::\n:::\n\n\nWe see that we get a model accuracy of 95% which is very good.\n\n**Images used can be found [here](https://towardsdatascience.com/probability-fundamentals-of-machine-learning-part-1-a156b4703e69)**\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}